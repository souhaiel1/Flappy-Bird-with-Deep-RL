{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0726DI1W8RJ"
      },
      "source": [
        "# Project : beat flappy bird\n",
        "\n",
        "You may be familiar with the game [flappy bird](https://flappybird.io/). It is very simple: a bird moves at constant speed on the x axis and, to direct him, you can either push it up or let it fall at each step. The goal of the game is to go as far as possible.\n",
        "\n",
        "The goal for this project is as follow: design and train an agent which does the best possible score at flappy bird !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itbl2DXtiAAF"
      },
      "source": [
        "## Members\n",
        "\n",
        "- Souhaiel BEN SALEM\n",
        "- Charbel ABI HANA\n",
        "- Adrian GARNIER ARTIÃ‘ANO\n",
        "- Israfel SALAZAR REYES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiSc6BlkByJV"
      },
      "outputs": [],
      "source": [
        "#@title Installations  { form-width: \"30%\" }\n",
        "\n",
        "# This is just for the purpose of this colab. Please do not share a ssh\n",
        "# private key in real life, it is a really unsafe practice.\n",
        "GITHUB_PRIVATE_KEY = \"\"\"-----BEGIN OPENSSH PRIVATE KEY-----\n",
        "b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW\n",
        "QyNTUxOQAAACD5ow+qHLZLVosHfeGcGeJKQgwUlPYgoFliCEsshiFhXwAAALCn99V2p/fV\n",
        "dgAAAAtzc2gtZWQyNTUxOQAAACD5ow+qHLZLVosHfeGcGeJKQgwUlPYgoFliCEsshiFhXw\n",
        "AAAECJ+OOLQqiwINexx26mmQt6FL5xXYHRf9Jv2UzahlW0avmjD6octktWiwd94ZwZ4kpC\n",
        "DBSU9iCgWWIISyyGIWFfAAAAKm1yaXZpZXJlQG1yaXZpZXJlLW1hY2Jvb2twcm8ucm9hbS\n",
        "5pbnRlcm5hbAECAw==\n",
        "-----END OPENSSH PRIVATE KEY-----\n",
        "\"\"\"\n",
        "\n",
        "# Create the directory if it doesn't exist.\n",
        "! mkdir -p /root/.ssh\n",
        "# Write the key\n",
        "with open(\"/root/.ssh/id_ed25519\", \"w\") as f:\n",
        "  f.write(GITHUB_PRIVATE_KEY)\n",
        "# Add github.com to our known hosts\n",
        "! ssh-keyscan -t ed25519 github.com >> ~/.ssh/known_hosts\n",
        "# Restrict the key permissions, or else SSH will complain.\n",
        "! chmod go-rwx /root/.ssh/id_ed25519\n",
        "\n",
        "# Clone and install the RL Games repository\n",
        "! if [ -d \"rl_games\" ]; then echo \"rl_games directory exists.\"; else git clone git@github.com:Molugan/rl_games.git; fi\n",
        "! cd rl_games ; git pull;  pip install .\n",
        "\n",
        "# Other dependencies\n",
        "# If you just want to play your environment and does not intend to use either\n",
        "# jax or haiku you can comment this part.\n",
        "!pip install dm-acme[jax]\n",
        "!pip install dm-acme[tf]\n",
        "!pip install dm-haiku\n",
        "!pip install chex\n",
        "!pip install optax\n",
        "!pip install jax[cuda]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztgSWy0fYvY5"
      },
      "source": [
        "## The environment\n",
        "\n",
        "We will use the Flappy Bird environment defined in the deep_rl package. Let's have a closer look at it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMk5FLsimhhx"
      },
      "outputs": [],
      "source": [
        "from deep_rl.environments.flappy_bird import FlappyBird\n",
        "\n",
        "env = FlappyBird(\n",
        "        gravity=0.05,\n",
        "        force_push=0.1,\n",
        "        vx=0.05,\n",
        "        prob_new_bar=1,\n",
        "        invictus_mode=False,\n",
        "        max_height_bar=0.5,\n",
        "    )\n",
        "\n",
        "print(env.help)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOFXHhUYmhVL"
      },
      "source": [
        "For example let's interact with it a little bit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUEklG8l726x"
      },
      "outputs": [],
      "source": [
        "rows, cols = env.min_res\n",
        "print(f\"We should use at least {rows} rows and {cols} when rendering the environment\")\n",
        "\n",
        "obs_reset = env.reset()\n",
        "print(\"First observation when reseting the environment:\")\n",
        "print(obs_reset)\n",
        "print()\n",
        "\n",
        "print(\"Now, let's perform a few steps\\n\")\n",
        "\n",
        "print(\"Step 1: we let the bird fall\")\n",
        "obs, reward, done = env.step(0)\n",
        "print(f\"Observation: {obs}\")\n",
        "print(f\"Reward: {reward}\")\n",
        "print(f\"Game over: {done}\")\n",
        "print()\n",
        "\n",
        "print(\"Step 2: we push the bird up\")\n",
        "obs, reward, done = env.step(1)\n",
        "print(f\"Observation: {obs}\")\n",
        "print(f\"Reward: {reward}\")\n",
        "print(f\"Game over: {done}\")\n",
        "print()\n",
        "\n",
        "print(\"Step 3: we push the bird up again\")\n",
        "obs, reward, done = env.step(1)\n",
        "print(f\"Observation: {obs}\")\n",
        "print(f\"Reward: {reward}\")\n",
        "print(f\"Game over: {done}\")\n",
        "print()\n",
        "\n",
        "print(\"Step 4: we push the bird up again\")\n",
        "obs, reward, done = env.step(1)\n",
        "print(f\"Observation: {obs}\")\n",
        "print(f\"Reward: {reward}\")\n",
        "print(f\"Game over: {done}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOVy4waHAivf"
      },
      "source": [
        "To simplify typing a bit, the deep_rl package implements a new type `FlappyObs` which corresponds to a state of the flappy bird environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwQsyfBBAxiA"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "BarObs = Tuple[float, float, float, bool]\n",
        "BirdObs = Tuple[float, float, float]\n",
        "FlappyObs = Tuple[BirdObs, List[BarObs]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI7HQF1zhNcW"
      },
      "source": [
        "## Baseline\n",
        "\n",
        "We provide you with a simple baseline: the `StableAgent` which does nothing more than keeping the bird stable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-0xIH_5DB4f4"
      },
      "outputs": [],
      "source": [
        "from deep_rl.environments.flappy_bird import FlappyObs\n",
        "\n",
        "\n",
        "class StableAgent:\n",
        "    \"\"\"An agent which just keeps the bird stable.\"\"\"\n",
        "\n",
        "    def __init__(self, target_y: float = 0.5):\n",
        "        self._target_y = target_y\n",
        "\n",
        "    def sample_action(\n",
        "        self,\n",
        "        observation: FlappyObs,\n",
        "        evaluation: bool,\n",
        "    ) -> int:\n",
        "        _, y_bird, v_y_bird = observation[0]\n",
        "\n",
        "        if y_bird <= self._target_y and v_y_bird <= 0:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnX6ij-gCCIN"
      },
      "source": [
        "Let's see how a single runs works in practice with this agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7EHsNT6CG0e"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from deep_rl.terminal_renderer import BashRenderer\n",
        "from deep_rl.episode_runner import run_episode\n",
        "from deep_rl.project_values import PROJECT_FLAPPY_BIRD_ENV\n",
        "\n",
        "# We are going to render the environment !\n",
        "ROWS = 30\n",
        "COLS = 60\n",
        "# Because ipython sucks, I have not found a cleaner option to add\n",
        "# the refresher function\n",
        "renderer = BashRenderer(ROWS,\n",
        "                        COLS,\n",
        "                        clear_fn = lambda: clear_output(wait=True))\n",
        "\n",
        "# Flappy bird environment\n",
        "env = PROJECT_FLAPPY_BIRD_ENV\n",
        "\n",
        "# Our agent\n",
        "agent = StableAgent()\n",
        "\n",
        "# We run a single episode, with rendering, over a maximum of 100 steps\n",
        "run_episode(env,\n",
        "            agent,\n",
        "            max_steps=100,\n",
        "            renderer = renderer,\n",
        "            time_between_frame=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtqXMbKKDWYB"
      },
      "source": [
        "Without rendering now, let's see the average reward we can get over 100 episodes with this agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUbF1hIHDcAN"
      },
      "outputs": [],
      "source": [
        "from deep_rl.project_values import PROJECT_FLAPPY_BIRD_ENV\n",
        "from deep_rl.episode_runner import run_episode\n",
        "\n",
        "# Flappy bird environment\n",
        "env = PROJECT_FLAPPY_BIRD_ENV\n",
        "\n",
        "# Our agent\n",
        "agent = StableAgent()\n",
        "\n",
        "N_EPISODES = 100\n",
        "\n",
        "reward = 0\n",
        "for _ in range(N_EPISODES):\n",
        "    reward += run_episode(env, agent, max_steps=1000, renderer=None)\n",
        "\n",
        "reward /= N_EPISODES\n",
        "\n",
        "print(f\"Average reward over {N_EPISODES} episodes: {reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NX9TgKx_EirA"
      },
      "source": [
        "An now, we need to do much better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hyzsUZLnpHT"
      },
      "source": [
        "## Let's get to work !\n",
        "\n",
        "We will attempt to design and train an agent that performs the best possible score on Flappy bird. Here are the constraints:\n",
        "- if you chose a Deep learning algorithm, you must use jax and Haiku. Pytorch is not allowed for this project.\n",
        "- our agent should converge in less than an hour. \n",
        "- our agent must maximize the reward obtained over 100 episodes with a maximal number of 1000 steps per episode.\n",
        "\n",
        "On top of that, we will  plot and analyse the relevant curves showing the evolution of our training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ5dLRk_0YeP"
      },
      "source": [
        "### Agent's API\n",
        "\n",
        "Our agent will implement a method, `sample_action`, which takes two arguments as input, the observed state and wether or not it is in evaluation mode, and pick the action to perform. Appart from that, we can add any other method we want to our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI_OlisCAeUQ"
      },
      "source": [
        "In this project, the main method that proved to solve this environment is the DQN (Deeq Q Networks) methods. We use the DQN implementation already developed in our deep rl class and we add and edit it accordingly. The main addditions to the base DQN model implemented in class are:\n",
        "- Scheduler for the epsilon policy where $\\epsilon$ decays over time (with the number of iterations (epochs): $$\n",
        "\\epsilon =  \\begin{cases} \\epsilon_{start} - \\frac{episode_{i}}{\\epsilon_{decayLastFrame}} \\: \\text{if} \\: episode_{i} \\leq \\epsilon_{decayLastFrame} \\\\ \\epsilon_{final} \\end{cases}$$ \n",
        "- Scheduler for the target network parameter updates. We set a simple update at each $sync_{target}$ episodes. \n",
        "- We add a pre-processing stage on the observations generated by the environment in order to provide a rich representation of the state.\n",
        "\n",
        "In addition to the base DQN, we train the following:\n",
        "- Double DQN\n",
        "- Dueling DQN\n",
        "- Double Dueling DQN\n",
        "We obtain the best resutls on the **Double Dueling DQN** (check below for training and evaluation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDD3EIFOEpSC"
      },
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "import time\n",
        "\n",
        "import jax\n",
        "import chex\n",
        "import optax\n",
        "\n",
        "import numpy as onp\n",
        "import jax.numpy as jnp\n",
        "import haiku as hk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from dataclasses import dataclass, is_dataclass\n",
        "from typing import List, Tuple\n",
        "from operator import itemgetter\n",
        "\n",
        "from deep_rl.project_values import PROJECT_FLAPPY_BIRD_ENV\n",
        "from IPython.display import clear_output\n",
        "from deep_rl.terminal_renderer import BashRenderer\n",
        "from deep_rl.episode_runner import run_episode\n",
        "\n",
        "\n",
        "sns.set_style('darkgrid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHQ6rV_JEpSC"
      },
      "outputs": [],
      "source": [
        "@chex.dataclass\n",
        "class Transition:\n",
        "    state_t: chex.Array\n",
        "    action_t: chex.Array\n",
        "    reward_t: chex.Array\n",
        "    done_t: chex.Array\n",
        "    state_tp1: chex.Array\n",
        "\n",
        "@dataclass\n",
        "class EpisodeTrainingStatus:\n",
        "    episode_number: int\n",
        "    reward: float\n",
        "    training_time: float\n",
        "\n",
        "@chex.dataclass\n",
        "class LearnerState:\n",
        "    online_params: hk.Params\n",
        "    target_params: hk.Params\n",
        "    opt_state: optax.OptState\n",
        "\n",
        "@dataclass\n",
        "class Epsilon:\n",
        "    epsilon_decay_last_frame: int\n",
        "    epsilon_start: float\n",
        "    epsilon_final: float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f27LajCGEpSC"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store transition tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, buffer_capacity: int):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "        Args:\n",
        "            buffer_capacity (int): maximal number of tuples to store at once\n",
        "        \"\"\"\n",
        "        self._memory = list()\n",
        "        self._maxlen = buffer_capacity\n",
        "\n",
        "    @property\n",
        "    def size(self) -> int:\n",
        "        # Return the current number of elements in the buffer.\n",
        "        return len(self._memory)\n",
        "\n",
        "    def add(\n",
        "        self,\n",
        "        state_t: chex.Array,\n",
        "        action_t: chex.Array,\n",
        "        reward_t: chex.Array,\n",
        "        done_t: chex.Array,\n",
        "        state_tp1: chex.Array,\n",
        "    ) -> None:\n",
        "        \"\"\"Add a new transition to memory.\"\"\"\n",
        "\n",
        "        if self.size > self._maxlen:\n",
        "            self._memory = self._memory[1:]\n",
        "\n",
        "        self._memory.append(\n",
        "            Transition(\n",
        "                state_t=state_t,\n",
        "                action_t=action_t,\n",
        "                reward_t=reward_t,\n",
        "                done_t=done_t,\n",
        "                state_tp1=state_tp1,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def sample(self) -> Transition:\n",
        "        \"\"\"Randomly sample a transition from memory.\"\"\"\n",
        "        assert self._memory, \"replay buffer is unfilled\"\n",
        "        index = onp.random.randint(self.size)\n",
        "        return self._memory[index]\n",
        "\n",
        "class BatchedReplayBuffer(ReplayBuffer):\n",
        "    def sample_batch(self, batch_size) -> Transition:\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        assert (\n",
        "            len(self._memory) >= batch_size\n",
        "        ), \"Insuficient number of transitions in replay buffer\"\n",
        "\n",
        "        samples = [self.sample() for i in range(batch_size)]\n",
        "        kwargs = dict()\n",
        "        for attr in [\"state_t\", \"action_t\", \"reward_t\", \"done_t\", \"state_tp1\"]:\n",
        "            kwargs[attr] = onp.array([getattr(s, attr) for s in samples])\n",
        "        return Transition(**kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvTBn1mnHY_a"
      },
      "source": [
        "### Base DQN\n",
        "The basic DQN model is mainly explained in the report of our project. Essentially what we require is a function approximator for the $Q$-values which is our neural network, a replay buffer with uniform sampling and a target network which is updated according to our scheduler mentioned previously and finally, a scheduler for an epsilon-greedy strategy for sampling actions from our agent. The loss is expressed as: \n",
        "$$\n",
        "\\mathcal{L} = (Q(s, a) - y)^2, \\: \\text{where,} \\: \\\\\n",
        "y = \\begin{cases} r, \\: \\text{if episode has ended} \\\\ r + \\gamma\\max_{a'\\in A}{\\hat{Q}(s', a')} \\end{cases}\n",
        "$$. \n",
        "\n",
        "We use a simple convolutional neural network with 2 `Conv2D` layers to extract the features from the input and one `MLP` (fully connected) layer to output the $Q$-values for each action in the action space. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zG5A8I7EpSC"
      },
      "outputs": [],
      "source": [
        "def dqn_flappy_network(x: chex.Array, n_actions: int):\n",
        "    x = x[..., None]\n",
        "    model = hk.Sequential(\n",
        "    [\n",
        "        hk.Conv2D(32, kernel_shape=[2, 2], stride=2, padding=\"VALID\"),\n",
        "        jax.nn.relu,\n",
        "        hk.Conv2D(64, kernel_shape=[2, 2], stride=2, padding=\"VALID\"),\n",
        "        jax.nn.relu,\n",
        "        hk.Flatten(),\n",
        "        hk.nets.MLP([64, n_actions])\n",
        "    ])\n",
        "    \n",
        "    return model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7BihYUgIzFz"
      },
      "source": [
        "#### Input Observation Transformation\n",
        "\n",
        "We transform our observation from our environment to obtain the shape $(1+FOV, 4)$ where $FOV$ is a hyperparameter which essentially represents the number of bars visible by our agent. We add $1$ to the $FOV$ because we have one row for the agent state:\n",
        "$$\n",
        "[x, y, v_x, v_y]\n",
        "$$\n",
        "In addition, each bar has the following features:\n",
        "$$\n",
        "[presence, h_{distance}, v_{distance}, top]\n",
        "$$\n",
        "Where:\n",
        "- $presence$: `bool` $(1/0)$, denotes if a bar is present or \"seen\" by the agent.\n",
        "- $h_{distance}$: `float` $[-0.5, 0.5]$, denotes the vertical distance measured relatively from the agent. \n",
        "- $v_{distance}$: `float` $[0, 0.5]$, denotes the horizontal distance measured relatively from the agent.\n",
        "- $top$: `bool` $(1/0)$, denotes wether the seen bar is on the top or bottom position.\n",
        "\n",
        "The input state is always padded to obtain the mentioned shape to ensure a constant input to the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuyJSu50EpSD"
      },
      "outputs": [],
      "source": [
        "class DQN:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: FlappyBird,\n",
        "        net: hk.Module,\n",
        "        epsilon: Epsilon,\n",
        "        gamma: float,\n",
        "        learning_rate: float,\n",
        "        buffer_capacity: int,\n",
        "        min_buffer_capacity: int,\n",
        "        batch_size: int,\n",
        "        target_ema: float,\n",
        "        seed: int = 0,\n",
        "        fov:int = 4,\n",
        "        sync_target: int = 100\n",
        "    ) -> None:\n",
        "        \"\"\"Initializes the DQN agent.\n",
        "\n",
        "        Args:\n",
        "          env: input maze environment.\n",
        "          gamma: discount factor\n",
        "          eps: probability to perform a random exploration when picking a new action.\n",
        "          learning_rate: learning rate of the online network\n",
        "          buffer_capacity: capacity of the replay buffer\n",
        "          min_buffer_capacity: min buffer size before picking batches from the\n",
        "            replay buffer to update the online network\n",
        "          batch_size: batch size when updating the online network\n",
        "          target_ema: weight when updating the target network.\n",
        "          seed: seed of the random generator.\n",
        "        \"\"\"\n",
        "        self._env = env\n",
        "        self._learning_rate = learning_rate\n",
        "        self._gamma = gamma\n",
        "        self._batch_size = batch_size\n",
        "        self._target_ema = target_ema\n",
        "        self._Na = env.N_ACTIONS\n",
        "        self.fov = fov\n",
        "        self.net = net\n",
        "        self.epsilon = epsilon\n",
        "        \n",
        "        # Define the neural network for this agent\n",
        "        self._init, self._apply = hk.without_apply_rng(hk.transform(self._hk_qfunction))\n",
        "        # Jit the forward pass of the neural network for better performances\n",
        "        self.apply = jax.jit(self._apply)\n",
        "\n",
        "        # Also jit the update functiom\n",
        "        #self._update_fn = jax.jit(self._update_fn)\n",
        "        # Initialize the network's parameters\n",
        "        self._rng = jax.random.PRNGKey(seed)\n",
        "        self._rng, init_rng = jax.random.split(self._rng)\n",
        "        self._learner_state = self._init_state(init_rng)\n",
        "\n",
        "        # Initialize the replay buffer\n",
        "        self._min_buffer_capacity = min_buffer_capacity\n",
        "        self._buffer = BatchedReplayBuffer(buffer_capacity)\n",
        "\n",
        "        # Build a variable to store the last state observed by the agent\n",
        "        self._state = None\n",
        "        \n",
        "        # Keep number of episodes stored for timed target updates and epsilon updates\n",
        "        self.episode = 0        \n",
        "        self.sync_target = sync_target\n",
        "        \n",
        "    def _optimizer(self) -> optax.GradientTransformation:\n",
        "        return optax.adam(learning_rate=self._learning_rate)\n",
        "\n",
        "    def _hk_qfunction(self, state: chex.Array) -> chex.Array:\n",
        "        s = state\n",
        "        return self.net(s, self._Na)\n",
        "\n",
        "    def _init_state(self, rng: chex.PRNGKey) -> LearnerState:\n",
        "        \"\"\"Initialize the online parameters, the target parameters and the\n",
        "        optimizer's state.\"\"\"\n",
        "        dummy_step = pre_process_obs(self._env.reset(), self.fov)[None]\n",
        "\n",
        "        online_params = self._init(rng, dummy_step)\n",
        "        target_params = online_params\n",
        "        opt_state = self._optimizer().init(online_params)\n",
        "\n",
        "        return LearnerState(\n",
        "            online_params=online_params,\n",
        "            target_params=target_params,\n",
        "            opt_state=opt_state,\n",
        "        )\n",
        "\n",
        "    def _update_fn(\n",
        "        self,\n",
        "        state: LearnerState,\n",
        "        batch: Transition,\n",
        "    ) -> Tuple[chex.Array, LearnerState]:\n",
        "        \"\"\"Get the next learner state given the current batch of transitions.\n",
        "\n",
        "        Args:\n",
        "          state: learner state before update.\n",
        "          batch: batch of experiences (st, at, rt, done_t, stp1)\n",
        "        Returns:\n",
        "          loss, learner state after update\n",
        "        \"\"\"\n",
        "        # Compute gradients\n",
        "        loss, gradients = jax.value_and_grad(self.loss_fn)(\n",
        "            state.online_params,\n",
        "            state.target_params,\n",
        "            batch.state_t,\n",
        "            batch.action_t,\n",
        "            batch.reward_t,\n",
        "            batch.done_t,\n",
        "            batch.state_tp1,\n",
        "        )\n",
        "\n",
        "        # Apply gradients\n",
        "        updates, new_opt_state = self._optimizer().update(gradients, state.opt_state)\n",
        "        new_online_params = optax.apply_updates(state.online_params, updates)\n",
        "\n",
        "        # Update target network params as:\n",
        "        # target_params <- ema * target_params + (1 - ema) * online_params\n",
        "        \n",
        "        if  self.episode % self.sync_target == 0:\n",
        "            new_target_params = jax.tree_map(\n",
        "                lambda x, y: x + (1 - self._target_ema) * (y - x),\n",
        "                state.target_params,\n",
        "                new_online_params,\n",
        "            )\n",
        "        else:\n",
        "            new_target_params = state.target_params\n",
        "            \n",
        "        return loss, LearnerState(\n",
        "            online_params=new_online_params,\n",
        "            target_params=new_target_params,\n",
        "            opt_state=new_opt_state,\n",
        "        )\n",
        "\n",
        "    def loss_fn(\n",
        "        self,\n",
        "        online_params: hk.Params,\n",
        "        target_params: hk.Params,\n",
        "        state_t: chex.Array,\n",
        "        action_t: chex.Array,\n",
        "        reward_t: chex.Array,\n",
        "        done_t: chex.Array,\n",
        "        state_tp1: chex.Array,\n",
        "    ) -> chex.Array:\n",
        "        \"\"\"Computes the Q-learning loss\n",
        "\n",
        "        Args:\n",
        "          online_params: parameters of the online network\n",
        "          target_params: parameters of the target network\n",
        "          state_t: batch of observations at time t\n",
        "          action_t: batch of actions performed at time t\n",
        "          reward_t: batch of rewards obtained at time t\n",
        "          done_t: batch of end of episode status at time t\n",
        "          state_tp1: batch of states at time t+1\n",
        "        Returns:\n",
        "          The Q-learning loss.\n",
        "        \"\"\"\n",
        "        # Step one: compute the target Q-value for state t+1\n",
        "        q_tp1 = self._apply(target_params, state_tp1)\n",
        "\n",
        "        # We do not want to consider the Q-value of states that are done !\n",
        "        # For theses states, q(t+1) = 0\n",
        "        q_tp1 = (1.0 - done_t[..., None]) * q_tp1\n",
        "\n",
        "        # Now deduce the value of the target cumulative reward\n",
        "        y_t = reward_t + self._gamma * jnp.max(q_tp1, axis=1)  # Shape B\n",
        "\n",
        "        # Compute the online Q-value for state t\n",
        "        q_t = self._apply(online_params, state_t)  # Shape B , Na\n",
        "\n",
        "        # Ok, but we only want the Q value for the actions that have actually\n",
        "        # been played\n",
        "        q_at = jax.vmap(lambda idx, q: q[idx])(action_t, q_t)\n",
        "\n",
        "        # Compute the square error\n",
        "        error = (q_at - y_t) ** 2\n",
        "\n",
        "        # Deduce the loss\n",
        "        return jnp.mean(error)\n",
        "\n",
        "    def sample_action(\n",
        "        self,\n",
        "        state,\n",
        "        evaluation: bool\n",
        "    ) -> int:\n",
        "        \"\"\"Picks the next action using an epsilon greedy policy.\n",
        "\n",
        "        Args:\n",
        "          state: observed state.\n",
        "          eval: if True the agent is acting in evaluation mode (which means it only\n",
        "            acts according to the best policy it knows.)\n",
        "        \"\"\"\n",
        "        # Fill in this function to act using an epsilon-greedy policy.\n",
        "\n",
        "        epsilon = max(self.epsilon.epsilon_final, self.epsilon.epsilon_start -\n",
        "                      self.episode / self.epsilon.epsilon_decay_last_frame)\n",
        "        \n",
        "        if not evaluation and onp.random.uniform() < epsilon:\n",
        "            return onp.random.randint(self._Na)\n",
        "        else:\n",
        "            if isinstance(state, tuple):\n",
        "                state = pre_process_obs(state, self.fov)\n",
        "                return onp.argmax(\n",
        "                self._apply(self._learner_state.online_params, state[None]))\n",
        "\n",
        "    def observe(\n",
        "        self,\n",
        "        action_t: chex.Array,\n",
        "        reward_t: chex.Array,\n",
        "        done_t: chex.Array,\n",
        "        state_tp1: chex.Array,\n",
        "    ) -> None:\n",
        "        \n",
        "        if isinstance(state_tp1, tuple):\n",
        "            state_tp1 = pre_process_obs(state_tp1, self.fov)\n",
        "        self._buffer.add(self._state, action_t, reward_t, done_t, state_tp1)\n",
        "        self._state = state_tp1\n",
        "\n",
        "        # We update the agent if and only if we have enought state stored in\n",
        "        # memory.\n",
        "        if self._buffer.size >= self._min_buffer_capacity:\n",
        "            batch = self._buffer.sample_batch(self._batch_size)\n",
        "            loss, self._learner_state = self._update_fn(self._learner_state, batch)\n",
        "            return loss\n",
        "        return 0.0\n",
        "    \n",
        "    def first_observe(self, state: chex.Array) -> None:\n",
        "        self._state = pre_process_obs(state, self.fov)\n",
        "        self.episode += 1\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5jL6LONEpSD"
      },
      "source": [
        "### Double DQN\n",
        "\n",
        "In the Double DQN extension, the researchers from Deepmind in [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/pdf/1509.06461.pdf) demonstrated that the base DQN tends to overestimate the values for $Q$ which is harmful for the training and the overall performance sometimes resulting in a suboptimal policy. The cause of this tends to come from the max operation in the Bellman equation:\n",
        "$$\n",
        "Q(s_t, a_t) = r_t + \\gamma\\max_{a}{Q'(s_{t+1}, a)}\n",
        "$$\n",
        "Where $Q'(s_{t+1}, a)$ were the $Q$ values calculated by the target network. The authors of the paper proposed choosing actions for the next state using the trained network but taking values of $Q$ from the target netowrk. The new expression for the target $Q$-values will be:\n",
        "$$\n",
        "Q(s_t, a_t) =  r_t + \\gamma\\max_{a}{Q'(s_{t+1}, arg\\max_{a}Q(s_{t+1}, a)})\n",
        "$$\n",
        "\n",
        "The changes are done in the loss function method of the base DQN which is shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJd8G6lGEpSD"
      },
      "outputs": [],
      "source": [
        "class DoubleDQN(DQN):\n",
        "    def __init__(self,\n",
        "        env: FlappyBird,\n",
        "        net: hk.Module,\n",
        "        epsilon: Epsilon,\n",
        "        gamma: float,\n",
        "        learning_rate: float,\n",
        "        buffer_capacity: int,\n",
        "        min_buffer_capacity: int,\n",
        "        batch_size: int,\n",
        "        target_ema: float,\n",
        "        seed: int = 0,\n",
        "        fov:int = 4,\n",
        "        sync_target: int = 100):\n",
        "        \n",
        "        super(DoubleDQN, self).__init__(\n",
        "        env,\n",
        "        net,\n",
        "        epsilon,\n",
        "        gamma,\n",
        "        learning_rate,\n",
        "        buffer_capacity,\n",
        "        min_buffer_capacity,\n",
        "        batch_size,\n",
        "        target_ema,\n",
        "        seed,\n",
        "        fov,\n",
        "        sync_target)\n",
        "    \n",
        "    def loss_fn(\n",
        "        self,\n",
        "        online_params: hk.Params,\n",
        "        target_params: hk.Params,\n",
        "        state_t: chex.Array,\n",
        "        action_t: chex.Array,\n",
        "        reward_t: chex.Array,\n",
        "        done_t: chex.Array,\n",
        "        state_tp1: chex.Array,\n",
        "    ) -> chex.Array:\n",
        "\n",
        "        a_tp1 = jnp.argmax(jax.lax.stop_gradient(self._apply(online_params, state_tp1)), axis=-1)\n",
        "        q_tp1 = self._apply(target_params, state_tp1)[jnp.arange(a_tp1.shape[0]), a_tp1]\n",
        "\n",
        "        # We do not want to consider the Q-value of states that are done !\n",
        "        # For theses states, q(t+1) = 0\n",
        "        q_tp1 = (1.0 - done_t[..., None]) * q_tp1\n",
        "\n",
        "        # Now deduce the value of the target cumulative reward\n",
        "        y_t = reward_t + self._gamma * q_tp1  # Shape B\n",
        "        \n",
        "        \n",
        "        \n",
        "        # Compute the online Q-value for state t\n",
        "        q_t = self._apply(online_params, state_t)  # Shape B , Na\n",
        "\n",
        "        # Ok, but we only want the Q value for the actions that have actually\n",
        "        # been played\n",
        "        q_at = jax.vmap(lambda idx, q: q[idx])(action_t, q_t)\n",
        "\n",
        "        # Compute the square error\n",
        "        error = (q_at - y_t) ** 2\n",
        "\n",
        "        # Deduce the loss\n",
        "        return jnp.mean(error)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wfbSKtVEpSD"
      },
      "source": [
        "### Dueling DQN\n",
        "\n",
        "For the dueling DQN extension, we define an architecture that separates the value of a state and the advantage of the state action pair where the advantage is $A(s, a)$ and the value is $V(s)$ and the networks estimates per usual $Q(s, a) = V(s) + A(s, a)$. Convolutional features from the input are processed in two parallel ways; one that calculates the $V(s)$ prediction and the other $A(s, a)$ after which these values are added. More specifically, to ensure correct and stable learning, we need to calculate $Q(s, a) = V(s) + A(s, a) - \\frac{1}{N}\\sum_{k}{A(s, k)}$. All of the other parts of the DQN training process are kept the same. [Source](https://arxiv.org/pdf/1511.06581.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9xmB047EpSD"
      },
      "outputs": [],
      "source": [
        "def dueling_dqn_flappy_network(x: chex.Array, n_actions: int):\n",
        "    x = x[..., None]\n",
        "    conv = hk.Sequential(\n",
        "    [\n",
        "        hk.Conv2D(32, kernel_shape=[2, 2], stride=2, padding=\"VALID\"),\n",
        "        jax.nn.relu,\n",
        "        hk.Conv2D(64, kernel_shape=[2, 2], stride=2, padding=\"VALID\"),\n",
        "        jax.nn.relu,\n",
        "        hk.Flatten()\n",
        "        \n",
        "    ])\n",
        "    \n",
        "    fc_adv = hk.Sequential(\n",
        "    [\n",
        "        #hk.nets.MLP([1*1*64, 128]),\n",
        "        #jax.nn.relu,\n",
        "        hk.nets.MLP([1*1*64, n_actions]),\n",
        "        \n",
        "    ])\n",
        "    \n",
        "    fc_val = hk.Sequential(\n",
        "    [\n",
        "        #hk.nets.MLP([1*1*64, 128]),\n",
        "        #jax.nn.relu,\n",
        "        hk.nets.MLP([1*1*64, 1]),\n",
        "    ])\n",
        "    \n",
        "    def adv_val(x: chex.Array):\n",
        "        conv_out = conv(x)\n",
        "        return fc_adv(conv_out), fc_val(conv_out)\n",
        "    \n",
        "    adv, val = adv_val(x)\n",
        "    return val + (adv - adv.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej_AmtiyEpSD"
      },
      "source": [
        "### Double Dueling DQN\n",
        "\n",
        "The double dueling DQN extension works by using the dueling neural network approach coupled with the double loss. This is supposed to achieve the best of both worlds and is setup in the configuration in the training section below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gHfxq9_E-zT"
      },
      "source": [
        "## Environment\n",
        "\n",
        "You must use the following flappy bird environment from the deep_rl package.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEt5Y82yPw0q"
      },
      "outputs": [],
      "source": [
        "from deep_rl.project_values import PROJECT_FLAPPY_BIRD_ENV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnY-RTgwELdS"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "You can use the following training loop to train your agent. Do not hesitate to play with the different parameters or even modify the code if you think you have a better option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdoO0UY92s2o"
      },
      "outputs": [],
      "source": [
        "MAX_TIME_TRAINING = 3600 * 2\n",
        "\n",
        "def run_episode_no_rendering(\n",
        "    env,\n",
        "    agent,\n",
        "    evaluation: bool,\n",
        "    max_steps: int,\n",
        ") -> float:\n",
        "    \"\"\"Runs a single episode.\n",
        "\n",
        "    Args:\n",
        "    env: environment to consider.\n",
        "    agent: agent to run.\n",
        "    evaluation: if False, will train the agent.\n",
        "    max_steps: number of steps after wich the evaluation should be stoppped\n",
        "      no matter what.\n",
        "    Returns:\n",
        "    The total reward accumulated over the episode.\n",
        "    \"\"\"\n",
        "\n",
        "    observation = env.reset()\n",
        "    agent.first_observe(state=observation)\n",
        "    tot_reward = 0\n",
        "\n",
        "    for i in range(max_steps):\n",
        "        action = agent.sample_action(observation, evaluation)\n",
        "        observation, reward, end_game = env.step(action)\n",
        "\n",
        "        if not evaluation:\n",
        "            agent.observe(action, reward, end_game, observation)\n",
        "        tot_reward += reward\n",
        "\n",
        "        if end_game:\n",
        "            break\n",
        "\n",
        "    return tot_reward\n",
        "\n",
        "\n",
        "def train_agent(\n",
        "    env,\n",
        "    agent,\n",
        "    num_episodes: int,\n",
        "    num_eval_episodes: int,\n",
        "    eval_every_N: int,\n",
        "    max_steps_episode: int,\n",
        "    max_time_training: float = MAX_TIME_TRAINING,\n",
        ") -> List[EpisodeTrainingStatus]:\n",
        "    \"\"\"Train your agent on the given environment.\n",
        "\n",
        "    Args:\n",
        "      env: environment to consider.\n",
        "      agent: agent to train.\n",
        "      num_episodes: number of episode to run for training.\n",
        "      eval_every_N: frequency at which the agent is evaluated.\n",
        "      max_steps_episode: maximal number of step per episode.\n",
        "      max_time_training: maximal duration of the training loop (in seconds).\n",
        "    Returns:\n",
        "      The total reward accumulated over the episode.\n",
        "    \"\"\"\n",
        "\n",
        "    all_status = []\n",
        "    print(f\"Episode number:\\t| Average reward on {num_eval_episodes} eval episodes\")\n",
        "    print(\"------------------------------------------------------\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        run_episode_no_rendering(\n",
        "            env, agent, evaluation=False, max_steps=max_steps_episode\n",
        "        )\n",
        "        if episode % eval_every_N == 0:\n",
        "            reward = 0\n",
        "            d_time = time.time() - start_time\n",
        "            for _ in range(num_eval_episodes):\n",
        "                reward += run_episode(\n",
        "                    env, agent, evaluation=True, max_steps=max_steps_episode\n",
        "                )\n",
        "            reward /= num_eval_episodes\n",
        "            print(f\"\\t{episode}\\t|\\t{reward}\")\n",
        "            all_status.append(\n",
        "                EpisodeTrainingStatus(\n",
        "                    episode_number=episode, reward=reward, training_time=d_time\n",
        "                )\n",
        "            )\n",
        "\n",
        "            if d_time > max_time_training:\n",
        "                break\n",
        "\n",
        "    return all_status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt21hZxaEpSE"
      },
      "outputs": [],
      "source": [
        "def pre_process_obs(obs: FlappyObs, FOV:int, sort=True):\n",
        "    \"\"\"\n",
        "    Function that pre-processes the observation:\n",
        "    \"\"\"\n",
        "    def sort_bars(bars: List[BarObs]):\n",
        "        return sorted(bars, key=itemgetter(0))\n",
        "    \n",
        "    def build_state(obs: List[FlappyObs]):\n",
        "        \"\"\"\n",
        "        Function that converts default observation into custom state\n",
        "        \"\"\"\n",
        "        agent, bars = obs\n",
        "\n",
        "        n_features = 4\n",
        "        new_bars = jnp.zeros(shape=(FOV, n_features))\n",
        "        \n",
        "        if len(bars):\n",
        "            if len(bars) > FOV:\n",
        "                bars = bars[:FOV]\n",
        "                \n",
        "            for i, bar in enumerate(bars):\n",
        "                x_min, x_max, height, top = bar\n",
        "                h_bar = x_min - agent[0]\n",
        "                if top:\n",
        "                    v_bar = (1 - height) - agent[1]\n",
        "                else:\n",
        "                    v_bar = height - agent[1]\n",
        "                new_bar = jnp.array([1., h_bar, v_bar, float(int(top))])\n",
        "                new_bars = new_bars.at[i].set(new_bar)\n",
        "                \n",
        "        new_agent = jnp.array([[agent[0], agent[1], 0.05, agent[2]]])\n",
        "\n",
        "        return jnp.concatenate([new_agent, new_bars])\n",
        "\n",
        "    bird, bars = obs\n",
        "    bird_x, bird_y, bird_vy = bird\n",
        "    \n",
        "    new_bars = []\n",
        "        \n",
        "    if len(bars):\n",
        "        for bar in bars:\n",
        "            x_min, x_max, h, top = bar\n",
        "            # remove bars which are behind the agent\n",
        "            if x_max >= bird_x:\n",
        "                new_bars.append(bar)\n",
        "        if sort:\n",
        "            # sort bars\n",
        "            new_bars = sort_bars(new_bars)\n",
        "        \n",
        "    new_obs: FlappyObs = (bird, new_bars)\n",
        "    \n",
        "    state = build_state(new_obs)\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8Ri0Jy3EpSE"
      },
      "source": [
        "### Trainings\n",
        "\n",
        "In this section, we only show the best configs obtained during our hyperparameters sweeps. We mention in the project report the sweeps of parameters and their general effects on the training and overall performance. We include in the project files the results obtained which are shown in report as \".npy\" files where each file has a shape of $(3, 100)$ where on the first dimension we have:\n",
        "- index 0: array of rewards\n",
        "- index 1: array of episodes\n",
        "- index 2: array of timesteps\n",
        "\n",
        "We include a section in the code to load the numpy files and visualize the results or just directly load the calculated ones at runtime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzYUG2bQEpSE"
      },
      "outputs": [],
      "source": [
        "NUM_EPISODES = 1200 # can be 1500 for ~ 2hrs, 1000 finishes under under 2hrs ~1hr 30 minutes per model. \n",
        "NUM_EVAL_EPISODES = 50\n",
        "EVAL_EVERY_N = 50\n",
        "MAX_STEPS_EPISODE = 100\n",
        "MAX_TIME_TRAINING = 3600 * 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xjx1ae20EpSE"
      },
      "outputs": [],
      "source": [
        "epsilon = Epsilon(epsilon_decay_last_frame= NUM_EPISODES, epsilon_start=0.99, epsilon_final=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XP37vOyEpSF"
      },
      "source": [
        "#### DQN - Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKTDPfrOEpSF",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "agent_1 = DQN(\n",
        "    env,\n",
        "    net=dqn_flappy_network,\n",
        "    epsilon=epsilon,\n",
        "    gamma=0.8,\n",
        "    learning_rate=1e-4,\n",
        "    buffer_capacity=1000,\n",
        "    min_buffer_capacity=32,\n",
        "    batch_size=32,\n",
        "    target_ema=0.9,\n",
        "    sync_target=1,\n",
        "    fov=4\n",
        ")\n",
        "all_status_dqn_1 = train_agent(\n",
        "    env=env,\n",
        "    agent=agent_1,\n",
        "    num_episodes=NUM_EPISODES,\n",
        "    num_eval_episodes=NUM_EVAL_EPISODES,\n",
        "    eval_every_N=EVAL_EVERY_N,\n",
        "    max_steps_episode=MAX_STEPS_EPISODE,\n",
        "    max_time_training=MAX_TIME_TRAINING,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i48Fgue8EpSF"
      },
      "source": [
        "#### Dueling DQN - Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJATDZHUEpSF",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "agent_dueling_1 = DQN(\n",
        "    env,\n",
        "    net=dueling_dqn_flappy_network,\n",
        "    epsilon=epsilon,\n",
        "    gamma=0.8,\n",
        "    learning_rate=3e-4,\n",
        "    buffer_capacity=1000,\n",
        "    min_buffer_capacity=32,\n",
        "    batch_size=32,\n",
        "    target_ema=0.9,\n",
        "    sync_target=1,\n",
        "    fov=4\n",
        ")\n",
        "all_status_dueling_1 = train_agent(\n",
        "    env=env,\n",
        "    agent=agent_dueling_1,\n",
        "    num_episodes=NUM_EPISODES,\n",
        "    num_eval_episodes=NUM_EVAL_EPISODES,\n",
        "    eval_every_N=EVAL_EVERY_N,\n",
        "    max_steps_episode=MAX_STEPS_EPISODE,\n",
        "    max_time_training=MAX_TIME_TRAINING,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZFZ8JYFEpSF"
      },
      "source": [
        "#### Double DQN - Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iq0G99XNEpSF",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "agent_double_1 = DoubleDQN(\n",
        "    env,\n",
        "    net=dqn_flappy_network,\n",
        "    epsilon=epsilon,\n",
        "    gamma=0.8,\n",
        "    learning_rate=3e-4,\n",
        "    buffer_capacity=1000,\n",
        "    min_buffer_capacity=32,\n",
        "    batch_size=32,\n",
        "    target_ema=0.9,\n",
        "    sync_target=1,\n",
        "    fov=4\n",
        ")\n",
        "all_status_double_1 = train_agent(\n",
        "    env=env,\n",
        "    agent=agent_double_1,\n",
        "    num_episodes=NUM_EPISODES,\n",
        "    num_eval_episodes=NUM_EVAL_EPISODES,\n",
        "    eval_every_N=EVAL_EVERY_N,\n",
        "    max_steps_episode=MAX_STEPS_EPISODE,\n",
        "    max_time_training=MAX_TIME_TRAINING,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJMMZ0wSEpSF"
      },
      "source": [
        "#### Double Dueling DQN Training - ! Best Results !"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWCLkIkOEpSF",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "agent_double_dueling_1 = DoubleDQN(\n",
        "    env,\n",
        "    net=dueling_dqn_flappy_network,\n",
        "    epsilon=epsilon,\n",
        "    gamma=0.8,\n",
        "    learning_rate=3e-4,\n",
        "    buffer_capacity=1000,\n",
        "    min_buffer_capacity=32,\n",
        "    batch_size=32,\n",
        "    target_ema=0.9,\n",
        "    sync_target=1,\n",
        "    fov=4\n",
        ")\n",
        "all_status_double_dueling = train_agent(\n",
        "    env=env,\n",
        "    agent=agent_double_dueling_1,\n",
        "    num_episodes=NUM_EPISODES,\n",
        "    num_eval_episodes=NUM_EVAL_EPISODES,\n",
        "    eval_every_N=EVAL_EVERY_N,\n",
        "    max_steps_episode=MAX_STEPS_EPISODE,\n",
        "    max_time_training=MAX_TIME_TRAINING,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOo3iz3qEpSG"
      },
      "source": [
        "### Visualizations + Results\n",
        "You can use the following code to visualize a single run made by your agent. This can help you for debugging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-Up8SeR9rZk"
      },
      "source": [
        "In this section, we show the visuals used in the paper. We show the methods used for the generated results and we also provide with the original numpy arrays used to generate the plots. We can either load the arrays from memory or import them from local machine (or drive)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTssT7c8EpSG"
      },
      "outputs": [],
      "source": [
        "def get_arr_from_status(status: List[EpisodeTrainingStatus]):\n",
        "    r_arr = onp.zeros((1, len(status)))\n",
        "    ep_arr = onp.zeros((1, len(status)))\n",
        "    t_arr = onp.zeros((1, len(status)))\n",
        "\n",
        "    for i, ep_status in enumerate(status):\n",
        "        r_arr[0, i] = ep_status.reward\n",
        "        ep_arr[0, i] = ep_status.episode_number\n",
        "        t_arr[0, i] = ep_status.training_time\n",
        "\n",
        "    status_arr = onp.concatenate([r_arr, ep_arr, t_arr])\n",
        "    return status_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aULXr1HmHz7x"
      },
      "outputs": [],
      "source": [
        "# uncomment to plot and visualize current runs metrics - else load saved numpy files (original data)\n",
        "#dqn_arr = get_arr_from_status(all_status_dqn_1)\n",
        "#dueling_arr = get_arr_from_status(all_status_dueling_1)\n",
        "#double_arr = get_arr_from_status(all_status_double_1)\n",
        "#dueling_double_arr = get_arr_from_status(all_status_double_dueling)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkFxfqUyH1bv"
      },
      "outputs": [],
      "source": [
        "#onp.save(\"all_status_dqn_1.npy\", dqn_arr)\n",
        "#onp.save(\"all_status_dueling_1.npy\", dueling_arr)\n",
        "#onp.save(\"all_status_double_1.npy\", double_arr)\n",
        "#onp.save(\"all_status_dueling_double_1.npy\", dueling_double_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0vvibR-EpSG"
      },
      "outputs": [],
      "source": [
        "def get_rolling_vals(x: onp.array, window_width:int=6) -> Tuple[onp.array]:\n",
        "    \n",
        "    ma_vec = onp.convolve(x, onp.ones((window_width,))/window_width, mode='valid')\n",
        "    \n",
        "    nrows = x.size - window_width + 1\n",
        "    n = x.strides[0]\n",
        "    \n",
        "    x2D = onp.lib.stride_tricks.as_strided(x, shape=(nrows, window_width), strides=(n, n))\n",
        "    mstd_vec = onp.std(x2D, axis=1)\n",
        "    \n",
        "    high = ma_vec + mstd_vec\n",
        "    low = ma_vec - mstd_vec\n",
        "    \n",
        "    return ma_vec, high, low"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXuiV1nXEpSG"
      },
      "outputs": [],
      "source": [
        "def plot_results(arrays: List[onp.array], labels: List[str], rolling_window:int = 6, show_std:bool = True):\n",
        "    \n",
        "    fig = plt.figure(figsize=(11, 7))\n",
        "    ax1 = fig.add_subplot(111)\n",
        "\n",
        "    for i, array in enumerate(arrays):\n",
        "        arr_ma, arr_hi, arr_lo = get_rolling_vals(array, window_width=rolling_window)\n",
        "\n",
        "        ax1.plot(arr_ma, label=labels[i])\n",
        "        if show_std:\n",
        "            ax1.fill_between(onp.arange(arr_ma.shape[0]), arr_hi, arr_lo, alpha=0.1)\n",
        "\n",
        "    \n",
        "    ticks = onp.arange(0, array.shape[0]+EVAL_EVERY_N, EVAL_EVERY_N)\n",
        "    ax1.set_xticks(ticks)\n",
        "    ax1.set_xticklabels([str(10*x) for x in ticks])\n",
        "    ax1.set_xlabel(\"Episodes\")\n",
        "    ax1.set_ylabel(\"Reward\")\n",
        "    ax1.set_title(\"Moving Average Reward of DQN + Extensions Over Nb of Episodes\")\n",
        "    ax1.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2yGHfZcEpSG"
      },
      "outputs": [],
      "source": [
        "# load numpy arrays containing metrics from original runs\n",
        "dqn_arr = onp.load(\"all_status_dqn_1.npy\")\n",
        "dueling_arr = onp.load(\"all_status_dueling_1.npy\") \n",
        "double_arr = onp.load(\"all_status_double_1.npy\")\n",
        "dueling_double_arr = onp.load(\"all_status_dueling_double_1.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWhQhWPWEpSG"
      },
      "outputs": [],
      "source": [
        "arrays = [arr[0] for arr in [dqn_arr, dueling_arr, double_arr, dueling_double_arr]]\n",
        "labels = [\"DQN\", \"DuelingDQN\", \"DoubleDQN\", \"DuelingDoubleDQN\"]\n",
        "\n",
        "plot_results(arrays, labels, rolling_window=10, show_std=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2N8TaN1EpSG"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(env, agent, n_episodes, max_steps):\n",
        "    rewards = onp.zeros(shape=(n_episodes,))\n",
        "    for i in range(n_episodes):\n",
        "        rewards[i] = run_episode(env, agent, max_steps=max_steps, renderer=None)\n",
        "\n",
        "    means = onp.mean(rewards)\n",
        "    stds = onp.std(rewards)\n",
        "    \n",
        "    print(\n",
        "        f\"Average reward over {n_episodes} episodes: {means:.3f} with standard deviation: {stds:.3f}\"\n",
        "    )\n",
        "    return rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-H9zhC3-EpSG"
      },
      "outputs": [],
      "source": [
        "print(\"Model: Base DQN\")\n",
        "_ = evaluate_model(env, agent_1, 100, 1000)\n",
        "\n",
        "print(\"Model: Dueling DQN\")\n",
        "_ = evaluate_model(env, agent_dueling_1, 100, 1000)\n",
        "\n",
        "print(\"Model: Double DQN\")\n",
        "_ = evaluate_model(env, agent_double_1, 100, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb8ZNpUMaKvg"
      },
      "outputs": [],
      "source": [
        "print(\"Model: Dueling Double DQN\")\n",
        "_ = evaluate_model(env, agent_double_dueling_1, 100, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAFyulxqEpSG"
      },
      "outputs": [],
      "source": [
        "print(\"Model: Stable Agent\")\n",
        "stable_rewards = evaluate_model(env, StableAgent(), 100, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSY-wQsHEpSG"
      },
      "source": [
        "### Best Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-84QiLQCEpSH"
      },
      "outputs": [],
      "source": [
        "# We are going to render the environment !\n",
        "ROWS = 30\n",
        "COLS = 60\n",
        "renderer = BashRenderer(ROWS,\n",
        "                        COLS,\n",
        "                        clear_fn= lambda: clear_output(wait=True))\n",
        "\n",
        "\n",
        "# We run a single episode, with rendering, over a maximum of 1000 steps\n",
        "run_episode(PROJECT_FLAPPY_BIRD_ENV,\n",
        "            agent_double_dueling_1,\n",
        "            max_steps= 1000,\n",
        "            renderer= renderer,\n",
        "            time_between_frame= 0.1, evaluation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2dRjZkSEpSH"
      },
      "outputs": [],
      "source": [
        "arrays = [dqn_arr[0], dueling_double_arr[0], stable_rewards]\n",
        "labels = [\"DQN\", \"DuelingDoubleDQN\", \"StableAgent\"]\n",
        "\n",
        "plot_results(arrays, labels, rolling_window=10, show_std=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQo3K5G2KTM6"
      },
      "source": [
        "From the results shown, we were able to train a Double Dueling DQN model in under 2 hours and obtain an average reward of $25$ as a results solving our environment (confirmed by the epic winner winner chicken dinner hidden message). We train for 1000 steps with evaluation on every 10 epochs. There still is a somewhat large standard deviation of the reward even though when evaluating we sample actions according to the greedy policy. This can come from the fact that the environment generation is sometimes way too hard and it might hint that the network still needs some training time. \n",
        "In addition, we haven't explored more extensions to the DQN method which include **Prioritized Replay Buffer**, **Noisy DQN** and **Rainbow DQN** which is a combination of most of the extensions shown and mentioned. "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
